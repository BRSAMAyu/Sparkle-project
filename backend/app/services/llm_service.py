from typing import List, Dict, AsyncGenerator, Optional
import asyncio
from loguru import logger

from app.config import settings
from app.services.llm.base import LLMProvider
from app.services.llm.providers import OpenAICompatibleProvider

# ==========================================
# ğŸ­ æ¼”ç¤ºæ¨¡å¼é¢„è®¾å“åº” (Demo Mock Responses)
# ==========================================
# ç”¨äºç«èµ›æ¼”ç¤ºï¼Œç¡®ä¿å…³é”®æµç¨‹ 100% æˆåŠŸä¸”ç§’å›
# è¦å¯ç”¨: åœ¨ .env ä¸­è®¾ç½® DEMO_MODE=true
#
# ğŸ’¡ ä½¿ç”¨è¯´æ˜:
# 1. åœ¨æ¼”ç¤ºè„šæœ¬ä¸­è¾“å…¥çš„æ–‡å­—å¿…é¡»ä¸ä¸‹é¢çš„ key å®Œå…¨ä¸€è‡´
# 2. å¯ä»¥æŒ‰éœ€æ·»åŠ æ›´å¤šå…³é”®è¯å’Œå“åº”
# ==========================================

DEMO_MOCK_RESPONSES: Dict[str, str] = {
    "å¸®æˆ‘åˆ¶å®šé«˜æ•°å¤ä¹ è®¡åˆ’": """å¥½çš„ï¼åŸºäºä½ çš„å­¦ä¹ æƒ…å†µï¼Œæˆ‘ä¸ºä½ åˆ¶å®šäº†ä¸€ä¸ªé«˜æ•ˆçš„é«˜æ•°å¤ä¹ è®¡åˆ’ã€‚

ğŸ“š **é«˜æ•°å†²åˆºå¤ä¹ è®¡åˆ’**

æ ¹æ®è‰¾å®¾æµ©æ–¯é—å¿˜æ›²çº¿å’Œä½ çš„çŸ¥è¯†æ˜Ÿå›¾åˆ†æï¼Œæˆ‘å‘ç°ä½ åœ¨ä»¥ä¸‹å‡ ä¸ªçŸ¥è¯†ç‚¹éœ€è¦é‡ç‚¹å¤ä¹ ï¼š

1. **æé™ä¸è¿ç»­** - æŒæ¡åº¦è¾ƒä½ï¼Œå»ºè®®ä¼˜å…ˆå¤ä¹ 
2. **å¯¼æ•°çš„åº”ç”¨** - éœ€è¦å¼ºåŒ–ï¼Œç‰¹åˆ«æ˜¯æœ€å€¼é—®é¢˜
3. **ç§¯åˆ†è®¡ç®—** - åŸºç¡€è¿˜ä¸é”™ï¼Œåšé¢˜å·©å›ºå³å¯

æˆ‘å·²ä¸ºä½ ç”Ÿæˆä»¥ä¸‹ä»»åŠ¡å¡ç‰‡ï¼š

```json
{
  "actions": [
    {
      "type": "create_task",
      "data": {
        "title": "æé™ä¸è¿ç»­é‡éš¾ç‚¹å¤ä¹ ",
        "type": "learning",
        "estimated_minutes": 45,
        "priority": "high"
      }
    },
    {
      "type": "create_task",
      "data": {
        "title": "å¯¼æ•°åº”ç”¨ä¸“é¢˜ç»ƒä¹ ",
        "type": "training",
        "estimated_minutes": 30,
        "priority": "medium"
      }
    },
    {
      "type": "create_task",
      "data": {
        "title": "ç§¯åˆ†è®¡ç®—åˆ·é¢˜",
        "type": "training",
        "estimated_minutes": 25,
        "priority": "normal"
      }
    }
  ]
}
```

å»ºè®®æŒ‰ç…§ä¸Šè¿°é¡ºåºå­¦ä¹ ï¼Œå…ˆæ”»å…‹å¼±é¡¹ï¼Œå†å·©å›ºå¼ºé¡¹ã€‚åŠ æ²¹ï¼ğŸ”¥""",

    "æˆ‘ä»Šå¤©è¦å­¦ä»€ä¹ˆ": """æ—©ä¸Šå¥½ï¼è®©æˆ‘çœ‹çœ‹ä½ çš„å­¦ä¹ çŠ¶æ€...

ğŸ“Š **ä»Šæ—¥å­¦ä¹ å»ºè®®**

æ ¹æ®ä½ çš„çŸ¥è¯†æ˜Ÿå›¾å’Œé—å¿˜æ›²çº¿åˆ†æï¼š

ğŸ”´ **éœ€è¦å¤ä¹ ** (æŒæ¡åº¦ä¸‹é™):
- çº¿æ€§ä»£æ•°ï¼šçŸ©é˜µè¿ç®— (è·ä¸Šæ¬¡å­¦ä¹ å·²è¿‡ 5 å¤©)
- é«˜æ•°ï¼šç§¯åˆ†æŠ€å·§ (æŒæ¡åº¦é™è‡³ 65%)

ğŸŸ¡ **ä»Šæ—¥æ¨èå­¦ä¹ **:
- æ¦‚ç‡è®ºï¼šæ¡ä»¶æ¦‚ç‡ (æŒ‰è®¡åˆ’åº”ä»Šæ—¥å­¦ä¹ )

ğŸ’¡ æˆ‘å»ºè®®ä½ ä»Šå¤©å…ˆèŠ± 20 åˆ†é’Ÿå¤ä¹ çº¿ä»£çŸ©é˜µè¿ç®—ï¼Œç„¶åå†å­¦ä¹ æ–°å†…å®¹ã€‚

éœ€è¦æˆ‘å¸®ä½ åˆ›å»ºä»Šæ—¥å­¦ä¹ ä»»åŠ¡å—ï¼Ÿ""",

    "è¿™é“é¢˜æ€ä¹ˆåš": """å¥½çš„ï¼Œè®©æˆ‘æ¥å¸®ä½ åˆ†æè¿™é“é¢˜ï¼

ğŸ“ **è§£é¢˜æ€è·¯**

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦è¯†åˆ«é¢˜ç›®çš„å…³é”®ä¿¡æ¯å’Œè€ƒæŸ¥çš„çŸ¥è¯†ç‚¹ã€‚

ä¸€èˆ¬æ¥è¯´ï¼Œè§£é¢˜å¯ä»¥åˆ†ä¸ºä»¥ä¸‹æ­¥éª¤ï¼š
1. **å®¡é¢˜** - æ˜ç¡®å·²çŸ¥æ¡ä»¶å’Œæ‰€æ±‚
2. **å»ºæ¨¡** - å»ºç«‹æ•°å­¦æ¨¡å‹æˆ–æ‰¾åˆ°é€‚ç”¨çš„å…¬å¼
3. **è®¡ç®—** - æŒ‰æ­¥éª¤è§„èŒƒè®¡ç®—
4. **éªŒè¯** - æ£€æŸ¥ç»“æœæ˜¯å¦åˆç†

å¦‚æœä½ èƒ½æŠŠå…·ä½“çš„é¢˜ç›®å‘ç»™æˆ‘ï¼Œæˆ‘å¯ä»¥ç»™ä½ æ›´è¯¦ç»†çš„è§£ç­”å’Œåˆ†æå“¦ï¼

ğŸ’¡ å°æç¤ºï¼šé‡åˆ°ä¸ä¼šçš„é¢˜ç›®ï¼Œå…ˆå°è¯•è‡ªå·±æ€è€ƒ 5 åˆ†é’Ÿï¼Œè¿™æ ·å­¦ä¹ æ•ˆæœæ›´å¥½ï¼""",
}


class LLMService:
    def __init__(self):
        self.provider: LLMProvider = OpenAICompatibleProvider(
            api_key=settings.LLM_API_KEY,
            base_url=settings.LLM_API_BASE_URL
        )
        self.default_model = settings.LLM_MODEL_NAME
        self.demo_mode = getattr(settings, 'DEMO_MODE', False)

    def _check_demo_match(self, messages: List[Dict[str, str]]) -> Optional[str]:
        """
        æ£€æŸ¥æ˜¯å¦åŒ¹é…æ¼”ç¤ºå…³é”®è¯

        Returns:
            åŒ¹é…çš„é¢„è®¾å“åº”ï¼Œå¦‚æœä¸åŒ¹é…åˆ™è¿”å› None
        """
        if not self.demo_mode:
            return None

        # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        user_content = ""
        for msg in reversed(messages):
            if msg.get("role") == "user":
                user_content = msg.get("content", "").strip()
                break

        if not user_content:
            return None

        # ç²¾ç¡®åŒ¹é…
        if user_content in DEMO_MOCK_RESPONSES:
            logger.info(f"âš¡ [DEMO MODE] Exact match for: {user_content}")
            return DEMO_MOCK_RESPONSES[user_content]

        # æ¨¡ç³ŠåŒ¹é… (åŒ…å«å…³é”®è¯)
        for key, response in DEMO_MOCK_RESPONSES.items():
            if key in user_content or user_content in key:
                logger.info(f"âš¡ [DEMO MODE] Fuzzy match for: {user_content} -> {key}")
                return response

        return None

    async def chat(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        temperature: float = 0.7,
        **kwargs
    ) -> str:
        """
        Send a chat request to the LLM.
        """
        # ğŸ­ Demo Mode æ‹¦æˆª
        mock_response = self._check_demo_match(messages)
        if mock_response:
            # æ¨¡æ‹Ÿæ€è€ƒå»¶è¿Ÿ
            await asyncio.sleep(1.0)
            return mock_response

        model = model or self.default_model
        logger.debug(f"Sending chat request to model: {model}")
        return await self.provider.chat(messages, model=model, temperature=temperature, **kwargs)

    async def stream_chat(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        temperature: float = 0.7,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """
        Stream chat response from the LLM.
        """
        # ğŸ­ Demo Mode æ‹¦æˆª - æµå¼è¿”å›é¢„è®¾å“åº”
        mock_response = self._check_demo_match(messages)
        if mock_response:
            # æ¨¡æ‹Ÿæµå¼è¾“å‡ºï¼Œæ¯æ¬¡è¾“å‡ºå‡ ä¸ªå­—ç¬¦
            chunk_size = 10
            for i in range(0, len(mock_response), chunk_size):
                chunk = mock_response[i:i + chunk_size]
                yield chunk
                # æ¨¡æ‹Ÿæ‰“å­—æ•ˆæœçš„å»¶è¿Ÿ
                await asyncio.sleep(0.03)
            return

        model = model or self.default_model
        logger.debug(f"Starting stream chat with model: {model}")
        async for chunk in self.provider.stream_chat(messages, model=model, temperature=temperature, **kwargs):
            yield chunk

# Singleton instance
llm_service = LLMService()
